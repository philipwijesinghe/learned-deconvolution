
# Experimentally unsupervised deconvolution for light-sheet microscopy

This repository contains code for experimentally unsupervised deconvolution for light-sheet microscopy with propagation-invariant beams.

This work was first presented in:

*Wijesinghe P., Corsetti S., Chow D.J.X., Sakata S., Dunning K.R., Dholakia K.* 
**Experimentally unsupervised deconvolution for light-sheet microscopy with propagation-invariant beams** 
(PREPRINT) 2022 [https://doi.org/10.1101/2021.05.26.445797](https://doi.org/10.1101/2021.05.26.445797)

### Changes
```
2022-07-22:     Now supports arbitrary PSFs, including experimentally measured PSFs (see PhysicsDataConfig.yml)
                Parallel threading to increase simulation speed
                Several bugfixes when training outside default parameters     
```

## Description

This code (based on PyTorch) trains a deep convolutional neural network (based on GANs) that is able to perform deconvolution and super-resolution of microscopy data that is encoded by an arbitrary known point-spread function (PSFs) (e.g., engineered PSFs, such as the Airy and Bessel beams.)
The network uses simulated paired images based on the known physics of light propagation.
It further uses unpaired real images for saliency checks, which reduces artefacts.

This code is separated into several functionalities/steps.
- Simulation of paired images using the principles of image formation in light-sheet microscopy (LSM)
  - Gaussian, Airy and Bessel-Gauss beams can be simulated from theory
  - Any other arbitrary PSF can also instead be provided as an input image (if simulated externally or measured experimentally)
- Network training using simulated paired and real unpaired LSM data (or simulated only)
- Inference (processing) of image sequences



### Acknowledgements

We acknowledge Erik Linder-Noren's [implementations of GANs in PyTorch](https://github.com/eriklindernoren/PyTorch-GAN) that instantiated the code developed in this project.


### License

This project is licensed under the terms of the MIT license. See LICENSE.md



## Workflow

### Note

It is important that simulated and real data used for training, as well as the widefield images used during inference, all have the same pixel size. 
It is recommended to select an appropriate pixel size in advance that matches the performance and desired output of the imaging system and network.
As a rule of thumb, a good pixel size is the Nyquist sampling size of the desired final resolution after deep learning.
For instance, if the system has a resolution of 2 um, the desired super-resolution is 1 um, then the image pixel size of the raw data and simulations should be around 0.5 um. 


### Simulate image pairs

To create simulated physics image pairs, the user manually creates a folder containing a ```PhysicsDataConfig.yml``` configuration file which lists all parameters and instructions to generate the data.
A template is available in the ```config_templates``` folder.
Data is generated by running ```prepare_physics_data.py``` with the folder path as a variable.


### Prepare experimental data for training (optional)

Network training can make use of real low-resolution unpaired experimental data to improve performance. 
This data should be provided as a folder containing a sequence of images that match the network input size (currently 64x64 pix).
These small images can be regions of interest from widefield images captured with your system (with matching pixel sizes).
This can be generated, for instance, using some cropping macro in ImageJ.
Images should be normalised and ideally stored as 8-bit png files.
A helper script is provided to automate this in ```prepare_real_data.py```, which automatically crops a sequence of ROIs from arbitrary sized image stacks (sequences).


### Train network

To train a new network, the user manually creates a folder containing a ```TrainConfig.yml``` configuration file which lists all training parameters. 
A template is available in the ```config_templates``` folder.
The configuration file must also include at least one path to simulated image pairs and at least one path to a folder containing real experimental images (ROIs). 
Real images must be 64x64 pixel PNG images.
Network training is initialised using ```train_physics_model.py```, and will populate the folder with training images to visualise training progress, and will periodically save the model in the ```/saved_model``` subfolder.

Network training without real microscopy images can be performed using ```train_simulation_model.py```.


### Inference

Widefield microscopy images can be processed using a trained network.
Widefield images should be a sequence of 2D PNG images of any size.
This is done by running ```process_trained.py``` with the directories of the widefield image sequence and also the associated path to the trained network ```/saved_model``` subfolder.



## Data and Demo

Data underpinning the publication is available at [https://doi.org/10.17630/bf92bc18-0b81-41f7-bd44-d74040af7cf0](https://doi.org/10.17630/bf92bc18-0b81-41f7-bd44-d74040af7cf0).

This data includes a demo dataset that illustrates the functionality of this code and the expected structure of the outputs.
The instructions to run the demo using the code included in this repository is included in the parent folder of the data.
The demo should take several hours to train, depending on the GPU. (2.5 hours on NVidia RTX 2060), and several seconds for inference.

### Reproduction

We note that perfect reproduction of trained models even with the same training data is not feasible.
This is because network training is stochastic and rarely deterministic. 
However, the inference of LSM image data with the included trained models will reproduce the outputs, which are also included.
Further, models trained with identical images and hyperparameters should exhibit equivalent performance to the included pre-trained models.



## Folder listings

```beams\```

Code to generate Gaussian, Airy and Bessel beam shapes


```config_templates\```

Templates of configuration files used for image pair simulation and network training


```deeplearn\```

Code to perform deep learning using PyTorch


```fileio\```

Supporting functions for reading and writing files


```lsm\```

Code to simulate LSM images and PSFs



## System requirements and installation

(Installation of all packages may take several hours.)

Deep learning uses **PyTorch**.
We have tested our code on Windows 10 and python 3.9 using the Anaconda environment.

Here, Anaconda (or miniconda) was installed and included in the Windows 10 PATH.
We can create a fresh Anacoda (python) environment (called 'DD' for Deep Deconvolution) using the command prompt.

```
conda create --name DD python
```
```
conda activate DD
```

PyTorch makes use of NVidia GPUs via the CUDA Toolkit [https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit), which must be installed if a GPU is to be used for training or inference.
It is recommended to use a cuda compatible GPU for training, because CPU-based training will be extremely slow. 

PyTorch can be installed following the instructions on [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
The CUDA version must match the toolkit version used by pytorch.

For example by running:
```
conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge
```

We make use of additional libraries:

```
conda install pillow matplotlib pyyaml scipy scikit-image joblib yaml imageio
```

This should provide the necessary environment for PyTorch with CUDA.

In python, the GPU Test should return ```true``` if using a compatible graphics card.

```python
import torch
torch.cuda.is_available()
```


### Package Version

This code has been tested to work on cuda 10.2 and 11.6 versions of pytorch and python 3.7--3.10.

The full package listing, including the version information, is included in ```env.md```




